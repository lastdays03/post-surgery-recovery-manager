import { LLMService } from './llm-service'
import { getContextForQuery } from './rag-search'
import { validateInput, validateResponse, SYSTEM_PROMPT_SAFETY_INSTRUCTION } from './safety-guardrails'
import { checkDailyLimit, logAIUsage, UsageLog } from './monitoring'
import { ChatMessage, ToolCall } from './types'
import { AI_TOOLS, TOOL_EXECUTORS } from './tools'

export interface ChatRequest {
    userId: string
    message: string
    conversationHistory?: ChatMessage[]
}

export interface ChatResponse {
    message: string
    error?: string
    sources?: string[]
    safetyWarning?: string
}

export async function chatWithAssistant(request: ChatRequest): Promise<ChatResponse> {
    const { userId, message } = request
    const startTime = Date.now()
    let endpoint = 'chat_agent'

    // 1. Safety & Validation (ì…ë ¥ ê²€ì¦)
    const validation = validateInput(message)
    if (!validation.isValid) {
        return { message: '', error: validation.error || validation.warning }
    }

    // ì‘ê¸‰ ìƒí™© ê°ì§€ ì‹œ ì¦‰ì‹œ ê²½ê³  ë°˜í™˜
    if (validation.requiresEmergencyWarning) {
        return {
            message: 'ğŸš¨ **ì‘ê¸‰ ìƒí™©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.**\n\nì¦‰ì‹œ 119ì— ì—°ë½í•˜ê±°ë‚˜ ê°€ì¥ ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤ì„ ë°©ë¬¸í•˜ì„¸ìš”. ì¶œí˜ˆ, í˜¸í¡ê³¤ë€, ì‹¬í•œ í†µì¦ ë“±ì€ ì¦‰ê°ì ì¸ ì˜ë£Œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.',
            safetyWarning: validation.warning
        }
    }

    // 2. Cost Limiter
    const canProceed = await checkDailyLimit(userId)
    if (!canProceed) {
        return { message: '', error: 'ì¼ì¼ AI ì‚¬ìš© í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì´ìš©í•´ì£¼ì„¸ìš”.' }
    }

    try {
        // 3. RAG Retrieval (Context Injection)
        const context = await getContextForQuery(message)
        const hasContext = context.length > 0

        // 4. Construct Initial Messages
        const systemMessage: ChatMessage = {
            role: 'system',
            content: `${SYSTEM_PROMPT_SAFETY_INSTRUCTION}

ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìˆ˜ìˆ  í›„ íšŒë³µì„ ë•ëŠ” 'íšŒë³µ ê´€ë¦¬ ë©”ì´íŠ¸'ì…ë‹ˆë‹¤.
ì´ì œ 'ì‹ìœ„ ì„¸ë¶„í™”(Diet Graduation)' ë° 'ì˜ì–‘ ê´€ë¦¬' ì „ë¬¸ê°€ë¡œì„œì˜ ë‹µë³€ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

[ì‹ë‹¨ ê´€ë¦¬ ì§€ì¹¨]
1. ì‚¬ìš©ìê°€ ë¨¹ì€ ìŒì‹ì„ ê¸°ë¡í•˜ê³  ì‹¶ì–´í•˜ë©´ 'analyze_meal_nutrition' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
2. í˜„ì¬ íšŒë³µ ë‹¨ê³„ì— ë§ëŠ” ìŒì‹ì„ ì¶”ì²œí•˜ë ¤ë©´ 'get_available_meals' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
3. ìˆ˜ìˆ  ì§í›„ë¼ë©´ ë°˜ë“œì‹œ 'get_recovery_protocol'ì„ í™•ì¸í•˜ì—¬ í—ˆìš©ëœ ìŒì‹ì¸ì§€ ê²€ì¦í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.
4. ì‚¬ìš©ìì˜ ìµœê·¼ ì†Œí™” ìƒíƒœë‚˜ í†µì¦ ê¸°ë¡('get_user_health_data')ì„ ì‹ë‹¨ ì œì•ˆì˜ ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
5. íŠ¹ì • ì‹ë‹¨ì„ ì¶”ì²œí•  ë•ŒëŠ” ë°˜ë“œì‹œ ë‹µë³€ ëì— í˜¹ì€ ì¤‘ê°„ì— ë‹¤ìŒ í˜•ì‹ì„ í¬í•¨í•˜ì„¸ìš”:
   <meal_suggestion>{"id": "...", "name": "...", "phase": "...", "nutrition": {...}, "ingredients": [...], "prepTime": 10}</meal_suggestion>
   (ìœ„ JSONì€ Meal ì¸í„°í˜ì´ìŠ¤ì™€ ì™„ë²½íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.)

[USER INFO]
User ID: ${userId}
`.trim()
        }

        let messages: ChatMessage[] = [
            systemMessage,
            ...(request.conversationHistory || []),
            { role: 'user', content: message }
        ]

        const client = LLMService.getClient()
        let retryCount = 0
        const MAX_RETRIES = 3

        while (retryCount < MAX_RETRIES) {
            // 5. LLM Inference
            const response = await client.chat({
                messages,
                temperature: 0.2,
                tools: AI_TOOLS
            })

            // 6. Handle Tool Calls
            if (response.toolCalls && response.toolCalls.length > 0) {
                messages.push({
                    role: 'assistant',
                    content: response.content || '',
                    tool_calls: response.toolCalls
                })

                for (const toolCall of response.toolCalls) {
                    const executor = TOOL_EXECUTORS[toolCall.function.name as keyof typeof TOOL_EXECUTORS]
                    if (executor) {
                        try {
                            const args = JSON.parse(toolCall.function.arguments)
                            // profileId(userId)ë¥¼ ìë™ìœ¼ë¡œ ì£¼ì…í•˜ê±°ë‚˜ ë³´ì • (ë³´ì•ˆìƒ ì´ìœ )
                            if (args.profileId && args.profileId !== userId) {
                                args.profileId = userId
                            }

                            const result = await executor(args)
                            messages.push({
                                role: 'tool',
                                tool_call_id: toolCall.id,
                                name: toolCall.function.name,
                                content: result
                            })
                        } catch (e: any) {
                            messages.push({
                                role: 'tool',
                                tool_call_id: toolCall.id,
                                name: toolCall.function.name,
                                content: JSON.stringify({ error: 'Tool execution failed', details: e.message })
                            })
                        }
                    }
                }
                retryCount++
                continue // ë„êµ¬ ê²°ê³¼ì™€ í•¨ê»˜ ë‹¤ì‹œ LLM í˜¸ì¶œ
            }

            // 7. Safety Validation (ì‚¬í›„ ê²€ì¦)
            const safetyCheck = validateResponse(response.content, message)

            if (!safetyCheck.isSafe) {
                console.warn('Safety violation detected:', safetyCheck.violations)

                // ìœ„ë°˜ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•ˆì „í•œ ëŒ€ì²´ ë©”ì‹œì§€ë¡œ êµì²´
                const fallbackMessage = safetyCheck.emergencyDetected
                    ? 'ğŸš¨ ì‘ê¸‰ ìƒí™©ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì¦‰ì‹œ 119ì— ì—°ë½í•˜ê±°ë‚˜ ê°€ì¥ ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤ì„ ë°©ë¬¸í•˜ì„¸ìš”.'
                    : 'ì£„ì†¡í•©ë‹ˆë‹¤. ì•ˆì „í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹´ë‹¹ ì˜ë£Œì§„ê³¼ ì§ì ‘ ìƒë‹´í•˜ì‹œëŠ” ê²ƒì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤.'

                await logAIUsage({
                    userId,
                    endpoint: 'chat_agent_safety_blocked',
                    model: 'llm-assistant',
                    inputTokens: response.usage.promptTokens,
                    outputTokens: response.usage.completionTokens,
                    latencyMs: Date.now() - startTime,
                    success: false,
                    errorMessage: `Safety violations: ${safetyCheck.violations.join(', ')}`
                })

                return {
                    message: fallbackMessage,
                    safetyWarning: safetyCheck.violations.join(' ')
                }
            }

            // 8. Success: Safe response
            const latency = Date.now() - startTime
            await logAIUsage({
                userId,
                endpoint: hasContext ? 'chat_rag_agent' : 'chat_agent',
                model: 'llm-assistant',
                inputTokens: response.usage.promptTokens,
                outputTokens: response.usage.completionTokens,
                latencyMs: latency,
                success: true
            })

            return {
                message: response.content
            }
        }

        throw new Error('Max retries for tool calls exceeded')

    } catch (error: any) {
        console.error('Chat Assistant Error:', error)
        await logAIUsage({
            userId,
            endpoint,
            model: 'unknown',
            inputTokens: 0,
            outputTokens: 0,
            latencyMs: Date.now() - startTime,
            success: false,
            errorMessage: error.message
        })

        return { message: '', error: 'AI ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' }
    }
}
